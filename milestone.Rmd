---
title: "Data Science Captsone Milestone Report"
author: "Shabbir Suterwala"
date: "Sunday, March 29, 2015"
output: html_document
---

# Data Science Captsone Milestone Report
**Shabbir Suterwala**

## Executive Summary
The purpose of this report is to establish an understanding of basic tasks required for data processing of a text corpus. English Blogs, News and Twitter corpora are analyised for word, line and unique word counts. Further more top 1gram, 2grams and 3grams are presented. 

## Data Analysis Level 1
The data was obtained from the link https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip, which is provided by the course instructures. The zip contains copora of blogs, news and twitter in 4 languages English, Dutch, French and Russian. The statistics of the combined corpous is as follows: 

```{r initialize,echo=TRUE,messages=FALSE}
source("milestone.R")
library(xtable)
```

```{r get_dataset_stats,echo=TRUE,messages=FALSE,cache=TRUE}
if (!exists("ds.stat")) {
    dss <- get_dataset_stats("Coursera-SwiftKey/final")
    dss$file <- gsub("Coursera-Swiftkey/final/.*/(..)_..(.*).txt", "\\1\\2",
                     as.character(dss$file), perl=TRUE, ignore.case=TRUE)
    dss[, c(2,3,4,6)] <- round(dss[, c(2,3,4,6)] / 1000000, digits = 4)
    names(dss) <- c("File", "Lines (mil)", "Words (mil)",
                    "Unique (mil)", "% Unique", "Bytes (MB)") 
    ds.stat <<- dss
} 
```
```{r echo=FALSE,results='asis'}
xt <- xtable(ds.stat)
print(xt, type='html')
```

## Data Preparation

We will primarily work with English dataset. Working with full English dataset took long time for the desktop computer. So we decided to break the data into a smaller set for this report: 

```{r breakdown_info,echo=TRUE}
breakdown = list(train=60, cv=20, test=20)
xt <- xtable(data.frame(Type=c("Training %", "CV %", "Test %"),
                        Project=unlist(breakdown), Report=c(5, NA, NA)))
```
```{r display_breakdown_info,echo=FALSE,results='asis'}
print(xt, type='html')
```

Thus the English training data for this report looks as follows:
```{r get_dataset,echo=TRUE,cache=TRUE}

train.dir <- sprintf("data.%02d", breakdown$train)
ds <- get_dataset(train.dir, breakdown)

if (!exists("ds.prep.stat")) {
    dps <- get_dataset_stats(train.dir)
    dps$file <- gsub(paste0(train.dir,"/.*/(..)_..(.*).txt.(.*)"), "\\1\\2 \\3",
                     as.character(dps$file),
                     perl=TRUE, ignore.case=TRUE)
    dps[, c(2,3,4,6)] <- round(dps[, c(2,3,4,6)] / 1000000, digits = 4)
    names(dps) <- c("File", "Lines (mil)", "Words (mil)",
                    "Unique (mil)", "% Unique", "Bytes (MB)")    
    ds.prep.stat <<- dps[grep("^en", dps$File),]
}
```
```{r display_dataset_info,echo=FALSE,results='asis'}
xt <- xtable(ds.prep.stat)
print(xt, type='html')
```

## Data Analysis

The 1-gram, 2-gram and 3-gram for Blogs, Twitter and News are as follows:
```{r make_ngrams,echo=TRUE,cache=TRUE}
make_ngrams(train.dir)
```
```{r show_ngrams,echo=TRUE,fig.height=20}
(freq.en.1grams <<- freq_ngrams(en.1grams, 100, "EN 1-Grams"))$plot
(freq.en.2grams <<- freq_ngrams(en.2grams, 100, "EN 2-Grams"))$plot
(freq.en.3grams <<- freq_ngrams(en.3grams, 100, "EN 3-Grams"))$plot
(freq.en.4grams <<- freq_ngrams(en.4grams, 100, "EN 4-Grams"))$plot
(freq.en.5grams <<- freq_ngrams(en.5grams, 100, "EN 5-Grams"))$plot

```


## Modeling Strategy

To buld the prediction model following tasks has to be further performed

   1. Use porter stemming to improve N-gram frequencies. This will help reduce data footprint of the model
   2. Use PCFG to understand the sentense structure. This will help with predicting the right POS
   3. Once probable words are found with combining porter stemming and PCFG, Use backoff strategy to predict probability of a given word.
   
## Logistic Strategy

Given the resource requirements for the tm package we will break the training data into multiple pieces and distribute this on multiple machines. Too keep things simple file system will be used as data sharing mechanism. 

   1. Training dataset will equally split into 10 parts. This can easily be done with Linux/Unix utilities.
   2. A total of 4 computers will be used to perform the tasks. Each computer will read the training set from folder that is shared and write the data to the shared folder. Tasks are:
      1. Creating Document Term Matrix for training part N
      2. Creating a merged model from individual models
   3. One master computer will issue the tasks. The master will will also execute tasks.

To reduce the data foot print other NLP smooting methods besides backoff smooting will be analzed and best will be picked
   
# Conclusion
English Blogs, News and Twitter corpora are analyised for word, line and unique word counts and corresponding 1gram, 2grams and 3grams are presented. Additionally, strategies for building a machine learning model to predict the next word is provided.

# References
1. http://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf
